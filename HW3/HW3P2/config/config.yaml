data_folder: './11785-f24-hw3p2/'
save_model_folder: './ckpt/'


train:
  lr: 2e-3
  epochs: 100
  warmup: 2
  batch_size: 64
  save_interval: 10
  weight_decay: 0.0001



# You may pass this as a parameter to the dataset class above
# This will help modularize your implementation
transforms: []  # Set of transformations
scheduler:
  T_max: 100
  patience: 3

specaug:
  freq_mask_param: 10
  time_mask_param: 100

model:
  input_size: 28
  embed_size: 1024

#  adaptive: false
#  adaptive_number_ratio: 0.04
#  adaptive_size_ratio: 0.04
#  max_n_time_masks: 20
#  apply_time_warp: true
#  apply_time_mask: true
#  apply_freq_mask: true
#  time_warp_window: 5
#  time_mask_width_range: [0, 40]
#  freq_mask_width_range: [0, 50]
#  num_freq_mask: 4
#  num_time_mask: 2

decode:
  # See https://github.com/flashlight/text/blob/main/flashlight/lib/text/decoder/LexiconDecoder.h#L20-L30
  # for what the options mean. Python binding exposes the same options from C++.
  # KenLM is a fast LM query implementation, and it can be powered by:
  # 1. official LibriSpeech 4-gram LM: the 4-gram.arpa file on http://www.openslr.org/11
  # 2. fairseq style, letter-based lexicon: https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst
  beam_width: 10

#resnet_embedding:
#  input_dim: 128
#  hidden_dim: 256
#  kernel_size: 3
#  stride: 1
#  padding: 1