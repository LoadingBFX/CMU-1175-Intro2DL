data_folder: './11785-f24-hw3p2/'
save_model_folder: './ckpt/'


train:
  lr: 2e-3
  epochs: 100
  warmup: 2
  batch_size: 64
  save_interval: 10
  weight_decay: 0.0001



# You may pass this as a parameter to the dataset class above
# This will help modularize your implementation
transforms: []  # Set of transformations
scheduler:
  T_max: 100
  patience: 3

specaug:
  freq_mask_param: 10
  time_mask_param: 100

model:
  input_size: 28
  embed_size: 1024

encoder:
  expand_dims: [128, 512]
  kernel_size: 7

pBLSTMs:
  dropout_prob: 0.3

decoder:
  dropout_prob: 0.5

decode:
  # See https://github.com/flashlight/text/blob/main/flashlight/lib/text/decoder/LexiconDecoder.h#L20-L30
  # for what the options mean. Python binding exposes the same options from C++.
  # KenLM is a fast LM query implementation, and it can be powered by:
  # 1. official LibriSpeech 4-gram LM: the 4-gram.arpa file on http://www.openslr.org/11
  # 2. fairseq style, letter-based lexicon: https://dl.fbaipublicfiles.com/fairseq/wav2vec/librispeech_lexicon.lst
  beam_width: 10

