Name: "dec_cond_lm"
DEBUG: True
###### Dataset -----------------------------------------------------------------
root                      : "./data/11785-fall-24-hw4p2-transformer-asr/hw4p2"                # TODO: Set the root path of your data
unpaired_text_partition   : "text-for-LM"               # unpaired text for LM pre-training
train_partition           : "train-clean-100"           # train-clean-100
val_partition             : "dev-clean"                 # validation partition
test_partition            : "test-clean"                # test partition
NUM_WORKERS               : 8
subset                    : 1        # Load a subset of the data (for debugging, testing, etc)
token_type                : "char"     # [char, 1k, 10k]
feat_type                 : 'fbank'    # ['fbank', 'mfcc']
num_feats                 : 80         # fbanks:[20-80], mfcc:[12:20]
batch_size                : 16
norm                      : 'cepstral' # ['global_mvn', 'cepstral']

###### SpecAugment ---------------------------------------------------------------
specaug                   : True
specaug_conf:
  apply_freq_mask         : True
  freq_mask_width_range   : 8
  num_freq_mask           : 4 # 2-4
  apply_time_mask         : True
  time_mask_width_range   : 30
  num_time_mask           : 8 #6-8

###### Network Specs -------------------------------------------------------------
d_model                   : 512
d_ff                      : 2048 # feed-forward hidden layer size mostly 4*d_model

###### Embedding Specs -----------------------------------------------------------
time_stride               : 4 # time-wise downsampling
feature_stride            : 2 # feature-wise downsampling
embed_dropout             : 0.3

###### Encoder Specs -------------------------------------------------------------
enc_dropout               : 0.2
enc_num_layers            : 9 # number of encoder layers
enc_num_heads             : 16

###### Decoder Specs -------------------------------------------------------------
dec_dropout               : 0.3
dec_num_layers            : 8 # number of decoder layers
dec_num_heads             : 16

###### Base Parameters -----------------------------------------------------------

use_ctc                   : True
ctc_weight                : 0.6
optimizer                 : "AdamW" # Adam, AdamW, SGD
momentum                  : 0.0
nesterov                  : True
learning_rate             : 1E-4
scheduler                 : "CosineAnnealing" # ['ReduceLR', 'CosineAnnealing']
factor                    : 0.2
patience                  : 2

e                         : 0
epochs                    : 60
pre_epochs                : 20

###### Wandb ----------------------------------------------------------
use_wandb                 : True
resume_logging            : False
pretrain                  : True

#### DecoderLM
